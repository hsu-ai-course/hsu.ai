{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "Today we will implement PageRank algorithm for a small collection of document about Information Retrieval. For this we will extract link information from Wikipedia and build a Google Matrix. \n",
    "\n",
    "To obtain ranking result you can use:\n",
    "- Naïve approach with matrix inversion\n",
    "- Power Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "pages = [\n",
    "    \"Bag-of-words model\",\n",
    "    \"Bayes' theorem\",\n",
    "    \"Cluster analysis\",\n",
    "    \"Content-based image retrieval\",\n",
    "    \"Database\",\n",
    "    \"Deep learning\",\n",
    "    \"Desktop search\",\n",
    "    \"Dimensionality reduction\",\n",
    "    \"Discounted Cumulative Gain\",\n",
    "    \"Eigenvector\",\n",
    "    \"Full-text search\",\n",
    "    \"Gummy bear\",\n",
    "    \"Hypertext\",\n",
    "    \"Image retrieval\",\n",
    "    \"Information system\",\n",
    "    \"Internet\",\n",
    "    \"K-nearest neighbors algorithm\",\n",
    "    \"Language model\",\n",
    "    \"Latent Dirichlet allocation\",\n",
    "    \"Latent semantic analysis\",\n",
    "    \"Low-rank approximation\",\n",
    "    \"Multimedia information retrieval\",\n",
    "    \"Netflix Prize\",\n",
    "    \"Netflix\",\n",
    "    \"Web query\",\n",
    "    \"Ranking (information retrieval)\",\n",
    "    \"Recommender systems\",\n",
    "    \"Relevance (information retrieval)\",\n",
    "    \"Rocchio algorithm\",\n",
    "    \"Search algorithm\",\n",
    "    \"Search engines\",\n",
    "    \"Semantic search\",\n",
    "    \"Semantic web\",\n",
    "    \"Sentiment analysis\",\n",
    "    \"Similarity search\",\n",
    "    \"Site search\",\n",
    "    \"Text mining\",\n",
    "    \"Text Retrieval Conference\",\n",
    "    \"Tf–idf\",\n",
    "    \"Vector space model\",\n",
    "    \"Web crawler\",\n",
    "    \"World Wide Web\"\n",
    "]\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "dataset = {}\n",
    "for page in tqdm.tqdm(pages):\n",
    "    if page not in dataset:\n",
    "        dataset[page] = wikipedia.page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential data is stored in adjacency matrix\n",
    "\n",
    "Here we create a 0/1 adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = np.zeros((len(pages), len(pages)))\n",
    "for j, page in enumerate(tqdm.tqdm(pages)):\n",
    "    for link in dataset[page].links:\n",
    "        if link in pages:\n",
    "            i = pages.index(link)\n",
    "            A[i, j] = 1\n",
    "\n",
    "plt.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a stochastic matrix M based on adjacency matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = A.copy()\n",
    "for j in range(len(pages)):\n",
    "    if sum(A[:, j]) > 0:\n",
    "        M[:, j] = A[:, j] / sum(A[:, j])\n",
    "    else:\n",
    "        M[:, j] = np.ones((len(pages),)) / len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(M)\n",
    "plt.show()\n",
    "print(f\"Values range from {M.min()} to {M.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare Google matrix\n",
    "\n",
    "$G_{ij}=\\alpha \\mathcal{M}_{ij}+(1-\\alpha ){\\frac {1}{N}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_google(M, alpha=0.85):\n",
    "    return M * alpha + np.ones(M.shape) * (1 - alpha) / M.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_google(M)\n",
    "print(f\"Values range from {G.min():.4f} to {G.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve naively\n",
    "\n",
    "$\\mathbf{R} =  (\\mathbf{I}-d \\mathcal{M})^{-1}  \\frac{1-d}{N}  \\mathbf{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(M.shape[0])\n",
    "d = 0.85\n",
    "N = M.shape[0]\n",
    "_1 = np.ones((M.shape[0], 1))\n",
    "\n",
    "R = np.matrix((I - d * M)).I @ _1 * ((1 - 0.85) / N)\n",
    "ids = np.argsort(-R.reshape(-1)).A1\n",
    "for idx in ids:\n",
    "    print(f\"{R[idx, 0]:.4f} {pages[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve with power method\n",
    "\n",
    "$R = G^{N}v_{random}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.rand(G.shape[0], 1)\n",
    "v /= sum(v)\n",
    "\n",
    "for i in range(35):\n",
    "    v = G @ v\n",
    "\n",
    "assert np.allclose(v, R), \"Estimated vector should be close to naively computed\"\n",
    "\n",
    "ids = np.argsort(-v.reshape(-1))\n",
    "for idx in ids:\n",
    "    print(f\"{v[idx, 0]:.4f} {pages[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Build in check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eig(G)\n",
    "evecs = abs(evecs)\n",
    "ids = np.argsort(-evecs[:, 0])\n",
    "for idx in ids:\n",
    "    print(f\"{evecs[idx, 0]:.4f} {pages[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
