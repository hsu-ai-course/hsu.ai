{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search using SIFT\n",
    "\n",
    "Let's think about information retrieval in the context of image search. How can we find images similar to a query in a fast way (faster than doing pair-wise comparison with all images in a database)? How can we identify same objects taken in slightly different contexts? \n",
    "\n",
    "One way to do this is to find special points of interest in every image, so called keypoints (or descriptors), which characterize the image and which are more or less invariant to scaling, orientation, illumination changes, and some other distortions. There are several algorithms available that identify such keypoints, and today we will focus on [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). \n",
    "\n",
    "Your task is to apply SIFT to a dataset of images and enable similar images search.\n",
    "\n",
    "## 1. Get dataset\n",
    "\n",
    "We will use `Caltech 101` dataset, download it from [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). It consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "# Colab specific code\n",
    "!gdown --id 137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(\"101_ObjectCategories.tar.gz\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SIFT example\n",
    "\n",
    "Below is an example how to extract SIFT keyponts using `opencv`. [This](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html) is a dedicated tutorial, and [this](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html) is another tutorial you may need to find matches between two images (use in your code `cv.drawMatches()` function to display keypoint matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_dir = '101_ObjectCategories'\n",
    "img = cv.imread(img_dir + '/gramophone/image_0018.jpg')\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "# the points\n",
    "kp = sift.detect(gray, None)\n",
    "\n",
    "# the points with vectors\n",
    "kp, descriptors = sift.detectAndCompute(gray, None)\n",
    "img = cv.drawKeypoints(gray, kp, img, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "print(\"Descriptors:\", descriptors.shape)\n",
    "print(\"[\", *descriptors[0], \"]\", sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Index of keypoints\n",
    "\n",
    "Let's suppose we've found image descriptors. How do we find similar images, having this information? In our case the descriptors are 128-dimensional vectors per keypoint, and there can be hundreds of such points. To enable fast search of similar images, you will index descriptors of all images using some data structure for approximate nearest neighbors search, such as Navigable Small World, Annoy, FAISS, .... Then, for a new (query) image you will compute its descriptors, and for each of them find `k` nearest neighbor descriptors from index (using Euclidean or Cosine distance). Finally, you will sort images (retrieved from neighbor descriptors) by frequency with which they appear in `k` nearest neighbors (more matches -- higher the rank).\n",
    "\n",
    "### 3.1. Build an index\n",
    "\n",
    "Read all images, saving category information. For every image generate SIFT descriptors and index them using HNSW from [`nmslib`](https://github.com/nmslib/nmslib), [FAISS](https://github.com/facebookresearch/faiss), [Annoy](https://github.com/spotify/annoy) or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 102/102 [01:31<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from tqdm import tqdm \n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "index = AnnoyIndex(128, 'angular')\n",
    "\n",
    "dataset = []\n",
    "for category in tqdm(listdir(img_dir)):\n",
    "    files = listdir(img_dir + \"/\" + category)\n",
    "    for file in files[:30]:\n",
    "        filename = img_dir + '/' + category + '/' + file\n",
    "        img = cv.imread(filename, cv.IMREAD_GRAYSCALE)\n",
    "        kp, descriptors = ... # TODO get descriptors here\n",
    "        \n",
    "        # take most significant\n",
    "        for vec in descriptors[:32]:\n",
    "        \n",
    "            # ADD A VECTOR TO AN INDEX\n",
    "            \n",
    "            # and then add image information to a dataset\n",
    "            dataset.append([category, filename])\n",
    "\n",
    "index.build(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Implement search function\n",
    "\n",
    "Implement a function which returns `k` neighbours (indices) sorted by similarity for a given query image name. You can use `collections.Counter.most_common(k)` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('101_ObjectCategories/strawberry/image_0022.jpg', 34),\n",
       " ('101_ObjectCategories/inline_skate/image_0023.jpg', 3),\n",
       " ('101_ObjectCategories/dollar_bill/image_0017.jpg', 2),\n",
       " ('101_ObjectCategories/barrel/image_0005.jpg', 2),\n",
       " ('101_ObjectCategories/brontosaurus/image_0003.jpg', 2)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def search(index, dataset, imagename, k):\n",
    "    img = ... # read an image\n",
    "    kp, descriptors = ... # find descriptors\n",
    "    \n",
    "    ## TODO: find\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# finds query image in the result, as it is indexed\n",
    "filename = img_dir + '/strawberry/image_0022.jpg'\n",
    "result = search(index, dataset, filename, 5)\n",
    "print(result)\n",
    "for r in result:\n",
    "    img = cv.imread(r[0], cv.IMREAD_GRAYSCALE)\n",
    "    print(r[0])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom\n",
    "\n",
    "# Images\n",
    "img = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
    "img = 'https://innopolis.university/upload/landing/c9d/image_3.1@2x.jpg'\n",
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "print(\"Result type\", type(results))\n",
    "pandas_detections_df = results.pandas().xyxy[0]\n",
    "\n",
    "pandas_detections_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check the model works for a Caltech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
