{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with English punkt: 1923\n",
      "Sentences with default method: 1923\n",
      "he rowed slowly and steadily toward where the bird was circling.\n",
      "['he', 'rowed', 'slowly', 'and', 'steadily', 'toward', 'where', 'the', 'bird', 'was', 'circling']\n",
      "['he', 'row', 'slowli', 'and', 'steadili', 'toward', 'where', 'the', 'bird', 'was', 'circl']\n"
     ]
    }
   ],
   "source": [
    "# if not downloaded yet\n",
    "# nltk.download('punkt')\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# try simple engish\n",
    "textfile = \"datasets/nlp/the old man and the sea.txt\"\n",
    "# or more complex\n",
    "# textfile = \"datasets/nlp/MenOfGoodWill.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "lexemes = []\n",
    "with open(textfile) as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "    # lets split text for sentences first\n",
    "    \n",
    "    # these 2 parts are the same. Either complex one:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print(\"Sentences with English punkt:\", len(sentences))\n",
    "    # or \"from the box\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    print(\"Sentences with default method:\", len(sentences))\n",
    "\n",
    "    \n",
    "    # let's explode sentences to lexemes\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        s_words = [word for word\n",
    "                    in tokenize.word_tokenize(sentence)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "        words.append(s_words)\n",
    "        lexemes.append(s_lexemes)\n",
    "\n",
    "# test\n",
    "print(sentences[400])\n",
    "print(words[400])\n",
    "print(lexemes[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words lexicon: 2498\n",
      "Words lexicon: 1925\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "lexicon1 = set(itertools.chain(*words))\n",
    "print(\"Words lexicon:\", len(lexicon1))\n",
    "\n",
    "lexicon2 = set(itertools.chain(*lexemes))\n",
    "print(\"Words lexicon:\", len(lexicon2))\n",
    "\n",
    "# will the numbers change if you change text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish {'fishing', 'fish', 'fishes', 'fished'}\n",
      "form {'formed', 'form', 'forms'}\n",
      "see {'sees', 'see', 'seeing'}\n",
      "come {'comes', 'come', 'coming'}\n",
      "carri {'carrying', 'carry', 'carried'}\n",
      "coil {'coiled', 'coil', 'coils'}\n",
      "sail {'sailed', 'sails', 'sailing', 'sail'}\n",
      "patch {'patches', 'patched', 'patch'}\n",
      "look {'look', 'looked', 'looking', 'looks'}\n",
      "back {'backs', 'backing', 'back', 'backed'}\n",
      "skin {'skinning', 'skinned', 'skin'}\n",
      "bring {'brings', 'bring', 'bringing'}\n",
      "love {'loved', 'love', 'lovely', 'loving'}\n",
      "stay {'stays', 'stay', 'stayed'}\n",
      "rememb {'remember', 'remembers', 'remembered'}\n",
      "know {'knows', 'know', 'knowing'}\n",
      "leav {'leaving', 'leave', 'leaves'}\n",
      "show {'showing', 'showed', 'show', 'shows'}\n",
      "drift {'drifted', 'drifting', 'drift'}\n",
      "plank {'planks', 'planking', 'plank'}\n",
      "end {'ended', 'end', 'ends'}\n",
      "wait {'waiting', 'wait', 'waited'}\n",
      "cut {'cut', 'cuts', 'cutting'}\n",
      "salt {'salt', 'salted', 'salting'}\n",
      "smell {'smelling', 'smelled', 'smell'}\n",
      "drop {'dropped', 'drop', 'dropping', 'drops'}\n",
      "think {'thinking', 'thinks', 'think'}\n",
      "get {'gets', 'getting', 'get'}\n",
      "play {'play', 'played', 'playing'}\n",
      "row {'rowing', 'rowed', 'row', 'rows'}\n",
      "kill {'killing', 'kill', 'killed', 'kills'}\n",
      "club {'clubbed', 'club', 'clubbing'}\n",
      "feel {'feelings', 'feel', 'feeling'}\n",
      "shiver {'shiver', 'shivering', 'shivered'}\n",
      "chop {'chop', 'chopping', 'chopped'}\n",
      "confid {'confident', 'confidence', 'confided'}\n",
      "bait {'baits', 'baited', 'bait'}\n",
      "rise {'rising', 'rises', 'rise'}\n",
      "thank {'thanked', 'thanks', 'thank'}\n",
      "ask {'asked', 'asking', 'ask'}\n",
      "shift {'shift', 'shifted', 'shifts', 'shifting'}\n",
      "want {'want', 'wants', 'wanted'}\n",
      "light {'lightness', 'lighted', 'light', 'lights', 'lightly'}\n",
      "tri {'trying', 'try', 'tried'}\n",
      "work {'worked', 'working', 'work'}\n",
      "hook {'hook', 'hooks', 'hooked'}\n",
      "strang {'strange', 'strangeness', 'strangely'}\n",
      "shoulder {'shoulder', 'shouldered', 'shoulders'}\n",
      "use {'used', 'using', 'use'}\n",
      "thought {'thoughts', 'thought', 'thoughtful'}\n",
      "open {'opened', 'opening', 'open'}\n",
      "place {'places', 'place', 'placed'}\n",
      "clean {'cleanly', 'clean', 'cleaned'}\n",
      "make {'makes', 'make', 'making'}\n",
      "pound {'pounded', 'pounds', 'pound'}\n",
      "keep {'keeps', 'keep', 'keeping'}\n",
      "lose {'lose', 'loses', 'losing'}\n",
      "care {'care', 'careful', 'carefully'}\n",
      "head {'heads', 'head', 'headed'}\n",
      "close {'closing', 'close', 'closed'}\n",
      "arm {'arms', 'armed', 'arm'}\n",
      "start {'started', 'start', 'starting'}\n",
      "live {'live', 'lived', 'lives'}\n",
      "set {'sets', 'set', 'setting'}\n",
      "need {'needed', 'needs', 'need'}\n",
      "mean {'mean', 'meaning', 'means'}\n",
      "drive {'drives', 'driving', 'drive'}\n",
      "hit {'hit', 'hits', 'hitting'}\n",
      "talk {'talk', 'talking', 'talked'}\n",
      "drink {'drinking', 'drinks', 'drink'}\n",
      "prove {'prove', 'proved', 'proving'}\n",
      "sleep {'sleeps', 'sleep', 'sleeping'}\n",
      "hard {'hardly', 'hardness', 'hard'}\n",
      "roll {'rolled', 'rolls', 'rolling'}\n",
      "dream {'dreams', 'dreaming', 'dreamed', 'dream'}\n",
      "fight {'fighting', 'fights', 'fight'}\n",
      "clear {'cleared', 'clear', 'clearing', 'clearly'}\n",
      "die {'dying', 'dies', 'die', 'died'}\n",
      "turn {'turned', 'turns', 'turn'}\n",
      "pull {'pulled', 'pulling', 'pull', 'pulls'}\n",
      "move {'moving', 'moved', 'move'}\n",
      "lash {'lashings', 'lash', 'lashing', 'lashed'}\n",
      "dip {'dip', 'dipping', 'dipped'}\n",
      "friend {'friend', 'friends', 'friendly'}\n",
      "swallow {'swallow', 'swallows', 'swallowing'}\n",
      "beauti {'beautiful', 'beautifully', 'beauty'}\n",
      "float {'floats', 'floating', 'floated'}\n",
      "solid {'solid', 'solidity', 'solidly'}\n",
      "tast {'taste', 'tasting', 'tasted'}\n",
      "loop {'loop', 'looped', 'loops'}\n",
      "watch {'watch', 'watched', 'watching'}\n",
      "circl {'circle', 'circling', 'circles', 'circled'}\n",
      "slant {'slant', 'slanting', 'slanted'}\n",
      "desper {'desperation', 'desperate', 'desperately'}\n",
      "rais {'raise', 'raised', 'raising'}\n",
      "cloud {'clouding', 'cloud', 'clouds'}\n",
      "poison {'poison', 'poisoning', 'poisonings'}\n",
      "rest {'rest', 'resting', 'rested'}\n",
      "step {'stepped', 'stepping', 'step'}\n",
      "hate {'hate', 'hateful', 'hated'}\n",
      "jump {'jumping', 'jumps', 'jumped', 'jump'}\n",
      "leap {'leaped', 'leaping', 'leap'}\n",
      "travel {'travel', 'travelling', 'travelled', 'travels'}\n",
      "increas {'increase', 'increased', 'increasing'}\n",
      "steer {'steered', 'steer', 'steering'}\n",
      "consid {'considering', 'considered', 'consider'}\n",
      "bump {'bumps', 'bumped', 'bumping'}\n",
      "pleas {'pleased', 'pleases', 'please'}\n",
      "slip {'slipped', 'slipping', 'slip'}\n",
      "gain {'gain', 'gained', 'gaining'}\n",
      "slow {'slow', 'slowed', 'slowing'}\n",
      "jerk {'jerk', 'jerking', 'jerked'}\n",
      "cramp {'cramping', 'cramps', 'cramp', 'cramped'}\n",
      "chang {'changes', 'change', 'changed'}\n",
      "dri {'dry', 'drying', 'dried'}\n",
      "comfort {'comfortable', 'comfort', 'comfortably'}\n",
      "prepar {'preparing', 'prepared', 'preparation', 'prepare'}\n",
      "sever {'several', 'sever', 'severed'}\n",
      "bleed {'bleeding', 'bleeds', 'bleed'}\n",
      "burn {'burn', 'burning', 'burned'}\n",
      "chew {'chew', 'chewed', 'chewing'}\n",
      "intellig {'intelligently', 'intelligent', 'intelligence'}\n",
      "pass {'passes', 'passed', 'pass'}\n",
      "gut {'gutted', 'guts', 'gut'}\n"
     ]
    }
   ],
   "source": [
    "clusters = defaultdict(set)\n",
    "for i in range(len(lexemes)):\n",
    "    for j in range(len(lexemes[i])):\n",
    "        clusters[lexemes[i][j]].add(words[i][j])        \n",
    "        # TODO: fill the dictionary or clusters\n",
    "        \n",
    "for key, value in clusters.items():\n",
    "    if len(value) > 2:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
