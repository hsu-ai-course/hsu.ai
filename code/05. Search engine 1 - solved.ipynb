{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def load_database(textfile):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    sentences = []\n",
    "    words = []\n",
    "    lexemes = []\n",
    "    with open(textfile) as f:\n",
    "        text = f.read().lower()\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            s_words = [word for word\n",
    "                        in tokenize.word_tokenize(sentence)\n",
    "                        if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                    ]\n",
    "            s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "            words.append(s_words)\n",
    "            lexemes.append(s_lexemes)\n",
    "    return sentences, words, lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, words, lexemes = load_database(\"datasets/nlp/facts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2stem(query):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(word) for word in tokenize.word_tokenize(query.lower())]\n",
    "\n",
    "# TODO: write the code that will find ALL sentences which contain all words of query\n",
    "def exact_match(query):\n",
    "    result = []\n",
    "    q = set(q2stem(query))\n",
    "    for i, sentence in enumerate(lexemes):\n",
    "        if set(sentence).intersection(q) == q:\n",
    "            result.append(sentences[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "# TODO: write the code that will find TOP sentences with THE BEST matches with query\n",
    "def ranked_match(query, top=5):\n",
    "    ranked_result = []\n",
    "    q = set(q2stem(query))\n",
    "    for i, sent in enumerate(lexemes):\n",
    "        ranked_result.append(\n",
    "            (\n",
    "                len(set(sent).intersection(q)),\n",
    "                sentences[i]\n",
    "            )\n",
    "        )\n",
    "    ranked_result.sort(reverse=True)\n",
    "    return [item[1] for item in ranked_result[:top]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_lookup(lexemes):\n",
    "    result = dict()\n",
    "    # for each text representation\n",
    "    for i, lx in enumerate(lexemes):\n",
    "        # for each lexeme in text\n",
    "        for lexeme in lx:\n",
    "            # if first met\n",
    "            if lexeme not in result: result[lexeme] = set()\n",
    "            # add text index to posting list\n",
    "            result[lexeme].add(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = construct_lookup(lexemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['english', 'languag']\n",
      "english {99, 155}\n",
      "languag {99}\n",
      "{99}\n",
      "94. of all the words in the english language, the word \"set\" has the most definitions.\n"
     ]
    }
   ],
   "source": [
    "q = \"english languages\"\n",
    "proc = q2stem(q)\n",
    "print(proc)\n",
    "res = None\n",
    "for lex in proc:\n",
    "    pl = index[lex]\n",
    "    print(lex, pl)\n",
    "    if res is None: res = pl\n",
    "    else: res = res & pl\n",
    "\n",
    "print(res)\n",
    "for i in res:\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94. of all the words in the english language, the word \"set\" has the most definitions.']\n",
      "================================\n",
      "['153. for every human on earth there are 1.6 million ants.', 'this means we have only seen 5% of the universe from earth.', 'that is the equivalent of a human jumping the empire state building.', '9. one in every five adults believe that aliens are hiding in our planet disguised as humans.', '88. earth is the only planet that is not named after a god.']\n"
     ]
    }
   ],
   "source": [
    "print(exact_match(\"english languages\"))\n",
    "print(\"================================\")\n",
    "print(ranked_match(\"humans earth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon size = 1925\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "lexicon = set(itertools.chain(*lexemes))\n",
    "inverted_lexicon = dict((word, i) for i, word in enumerate(lexicon))\n",
    "print(\"Lexicon size =\", len(lexicon))\n",
    "\n",
    "WF = [defaultdict(int) for _ in lexemes]\n",
    "DF = defaultdict(int)\n",
    "for i, doc in enumerate(lexemes):\n",
    "    for word in doc:\n",
    "        WF[i][word] += 1\n",
    "    for word in set(doc):\n",
    "        DF[word] += 1\n",
    "        \n",
    "\n",
    "def tf(word, doc_i):\n",
    "    return WF[doc_i][word] / len(lexemes[doc_i])\n",
    "\n",
    "\n",
    "def idf(word):\n",
    "    return -math.log(DF[word] / len(lexemes))\n",
    "\n",
    "\n",
    "# use the same method to buid a vector for documents (just use lookup) and for new queries\n",
    "def to_vector(tokens, i=None):\n",
    "    result = list([0] * len(lexicon))\n",
    "    if i is None:\n",
    "        local_tf = dict((word, tokens.count(word)) for word in set(tokens))    \n",
    "    for word in tokens:\n",
    "        if word in lexicon:\n",
    "            if i is None:\n",
    "                result[inverted_lexicon[word]] = local_tf[word] * idf(word)    \n",
    "            else:\n",
    "                result[inverted_lexicon[word]] = tf(word, i) * idf(word)\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_tdm(lexemes):\n",
    "    result = []\n",
    "    for i, sent in enumerate(lexemes):\n",
    "        result.append(to_vector(sent, i))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM = create_tdm(lexemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was the yellow gulf weed that had made so much phosphorescence in the night.',\n",
       " 'so he hooked a patch of yellow gulf weed with the gaff as they passed and shook it so that the small shrimps that were in it fell onto the planking of the skiff.',\n",
       " 'there was yellow weed on the line but the old man knew that only made an added drag and he was pleased.',\n",
       " 'just before it was dark, as they passed a great island of sargasso weed that heaved and swung in the light sea as though the ocean were making love with something under a yellow blanket, his small line was taken by a dolphin.',\n",
       " 'he saw the phosphorescence of the gulf weed in the water as he rowed over the part of the ocean that the fishermen called the great well because there was a sudden deep of seven hundred fathoms where all sorts of fish congregated because of the swirl the current made against the steep walls of the floor of the ocean.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import dot\n",
    "\n",
    "def search(query, top=5):\n",
    "    ranked_result = []\n",
    "    v = to_vector(q2stem(query))\n",
    "    for i, vect in enumerate(TDM):\n",
    "        ranked_result.append(\n",
    "            (\n",
    "                dot(v, vect) / (dot(v, v) * dot(vect, vect)), #  cosine\n",
    "                sentences[i]\n",
    "            )\n",
    "        )\n",
    "    ranked_result.sort(reverse=True)\n",
    "    return [item[1] for item in ranked_result[:top]]\n",
    "    \n",
    "search(\"yellow Gulf weed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
